# Databricks Apps ÂÆüË£ÖË®àÁîª - Êó¢Â≠òUIÈñãÁô∫ËÄÖÂêë„Åë

## üìä ÁèæÁä∂Ë©ï‰æ°

### ‚úÖ Êó¢„Å´ÂÆå‰∫Ü„Åó„Å¶„ÅÑ„ÇãÈÉ®ÂàÜ
- [x] Gradio UI„ÅÆÂü∫Êú¨ÂÆüË£Ö (TrainingForm, PredictionForm, SelectableTable)
- [x] „Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÊßãÈÄ† (components/, lib/, data/)
- [x] Âü∫Êú¨ÁöÑ„Å™APIÈÄö‰ø°Ê©üËÉΩ (apiClient.py)
- [x] Ë®≠ÂÆöÁÆ°ÁêÜ (settings.py)
- [x] „Éá„Éº„ÇøÂá¶ÁêÜ„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£

### ‚ùå Êú™ÂÆüË£Ö„Éª‰∏çÂÆåÂÖ®„Å™ÈÉ®ÂàÜ
- [ ] Databricks AppsË®≠ÂÆö„Éï„Ç°„Ç§„É´ (app.yaml)
- [ ] Áí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö (.env, .env.example)
- [ ] Databricks Jobs APIÂÆåÂÖ®Áµ±Âêà
- [ ] Databricks Notebook„ÅÆ‰ΩúÊàê
- [ ] CI/CD„Éë„Ç§„Éó„É©„Ç§„É≥
- [ ] Secret ScopeË®≠ÂÆö
- [ ] „ÉÜ„Çπ„Éà„Ç≥„Éº„Éâ
- [ ] „É≠„ÇÆ„É≥„Ç∞„Éª„É¢„Éã„Çø„É™„É≥„Ç∞Ê©üËÉΩ

## üéØ ÂÆüË£ÖË®àÁîª„ÅÆÂÖ®‰ΩìÂÉè

```
Phase 1: „É≠„Éº„Ç´„É´Áí∞Â¢ÉÊï¥ÂÇô (1-2Êó•)
  ‚îî‚îÄ> Phase 2: DatabricksÁµ±ÂêàÊ∫ñÂÇô (2-3Êó•)
       ‚îî‚îÄ> Phase 3: Databricks Apps „Éá„Éó„É≠„Ç§ (3-5Êó•)
            ‚îî‚îÄ> Phase 4: Êú¨Áï™Áµ±Âêà„Å®„ÉÜ„Çπ„Éà (5-7Êó•)
                 ‚îî‚îÄ> Phase 5: Êú¨Áï™„É™„É™„Éº„Çπ (1Êó•)
```

## üìÖ Phase 1: „É≠„Éº„Ç´„É´Áí∞Â¢ÉÊï¥ÂÇô (1-2Êó•)

### ÁõÆÊ®ô
Êó¢Â≠ò„ÅÆUI„Ç≥„Éº„Éâ„ÇíDatabricks AppsÂØæÂøú„Å´Ê∫ñÂÇô„Åó„ÄÅ„É≠„Éº„Ç´„É´„ÅßÂãï‰ΩúÁ¢∫Ë™ç„Åß„Åç„ÇãÁä∂ÊÖã„Å´„Åô„Çã

### „Çø„Çπ„ÇØ

#### 1.1 Áí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö„Éï„Ç°„Ç§„É´„ÅÆ‰ΩúÊàê ‚≠ê **ÊúÄÂÑ™ÂÖà**

**ÊâÄË¶ÅÊôÇÈñì**: 1ÊôÇÈñì

**‰ΩúÊàê„Éï„Ç°„Ç§„É´**:
```bash
PricingAIFrontend-develop 2/
‚îú‚îÄ‚îÄ .env.example          # „ÉÜ„É≥„Éó„É¨„Éº„ÉàÔºàGitÁÆ°ÁêÜÂØæË±°Ôºâ
‚îú‚îÄ‚îÄ .env.local.example    # „É≠„Éº„Ç´„É´ÈñãÁô∫Áî®„ÉÜ„É≥„Éó„É¨„Éº„Éà
‚îî‚îÄ‚îÄ config/
    ‚îî‚îÄ‚îÄ config.env        # ÂÖ±ÈÄöË®≠ÂÆöÔºàÈùûÊ©üÂØÜÊÉÖÂ†±„ÅÆ„ÅøÔºâ
```

**ÂÆüË£ÖÂÜÖÂÆπ**:
```bash
# .env.example
# Databricks API „Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà
POST_SOURCE_URL=https://adb-xxxxx.azuredatabricks.net/api/2.2/jobs/run-now
GET_PROGRESS_URL=https://adb-xxxxx.azuredatabricks.net/api/2.2/jobs/runs/get
POST_PRED_URL=https://adb-xxxxx.azuredatabricks.net/api/2.2/jobs/run-now
GET_RESULT_URL=https://adb-xxxxx.azuredatabricks.net/api/2.2/jobs/runs/get-output

# Databricks Jobs ID (Âæå„ÅßË®≠ÂÆö)
TRAINING_JOB_ID=
PREDICTION_JOB_ID=

# Databricks „Ç¢„ÇØ„Çª„Çπ„Éà„Éº„ÇØ„É≥ („É≠„Éº„Ç´„É´ÈñãÁô∫Áî® - ÂÆüÈöõ„ÅÆÂÄ§„ÅØ .env.local „Å´Ë®òËºâ)
DATABRICKS_TOKEN=

# Gradio Ë®≠ÂÆö
GRADIO_SERVER_NAME=0.0.0.0
GRADIO_SERVER_PORT=7860
GRADIO_ANALYTICS_ENABLED=False
```

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] .env.example ‰ΩúÊàê
- [ ] .env.local.example ‰ΩúÊàê
- [ ] .gitignore „Å´ .env.local ËøΩÂä†
- [ ] settings.py „ÅÆÁí∞Â¢ÉÂ§âÊï∞Ë™≠„ÅøËæº„ÅøÁ¢∫Ë™ç

---

#### 1.2 app.yaml „ÅÆ‰ΩúÊàê ‚≠ê **ÊúÄÂÑ™ÂÖà**

**ÊâÄË¶ÅÊôÇÈñì**: 1-2ÊôÇÈñì

**„Éï„Ç°„Ç§„É´„Éë„Çπ**: `PricingAIFrontend-develop 2/app.yaml`

**ÂÆüË£ÖÂÜÖÂÆπ**:
```yaml
# Databricks Apps Ë®≠ÂÆö„Éï„Ç°„Ç§„É´
name: pricing-ai-frontend
version: 1.0.0

# „Ç®„É≥„Éà„É™„Éº„Éù„Ç§„É≥„Éà
command:
  - python
  - app.py

# Áí∞Â¢ÉÂ§âÊï∞
env:
  GRADIO_SERVER_NAME: "0.0.0.0"
  GRADIO_SERVER_PORT: "8080"
  GRADIO_ANALYTICS_ENABLED: "False"
  PYTHONPATH: "/workspace"

# „É™„ÇΩ„Éº„ÇπË®≠ÂÆö
resources:
  driver:
    memory: "4g"
    cores: 2

# ‰æùÂ≠òÈñ¢‰øÇ
dependencies:
  python:
    requirements: requirements.txt

# „Éò„É´„Çπ„ÉÅ„Çß„ÉÉ„ÇØ
health_check:
  path: /
  interval: 30
  timeout: 10
  retries: 3

# „Éù„Éº„ÉàÂÖ¨Èñã
ports:
  - port: 8080
    protocol: http

# „Ç∑„Éº„ÇØ„É¨„ÉÉ„Éà (Phase 2„ÅßË®≠ÂÆö)
# secrets:
#   - scope: pricing-ai-secrets
#     key: databricks-token
#     env_var: DATABRICKS_TOKEN
```

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] app.yaml ‰ΩúÊàê
- [ ] requirements.txt „ÅÆ‰æùÂ≠òÈñ¢‰øÇÁ¢∫Ë™ç„ÉªÊõ¥Êñ∞
- [ ] app.py „ÅåDatabricks AppsÁí∞Â¢ÉÂ§âÊï∞„Çí‰ΩøÁî®„Åô„Çã„Çà„ÅÜ„Å´‰øÆÊ≠£

---

#### 1.3 app.py „ÅÆ Databricks Apps ÂØæÂøú‰øÆÊ≠£

**ÊâÄË¶ÅÊôÇÈñì**: 30ÂàÜ

**‰øÆÊ≠£ÂÜÖÂÆπ**:
```python
# app.py (‰øÆÊ≠£Áâà)
from components.layouts.MainLayout import MainLayout
from components.tables.SelectableTable import SelectableTable
from components.forms.PredictionForm import PredictionForm
from components.forms.TrainingForm import TrainingForm
from data.demodata import checkbox_demo_data
import gradio as gr
import os
import logging

# „É≠„ÇÆ„É≥„Ç∞Ë®≠ÂÆö
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Databricks Apps Áí∞Â¢ÉÂ§âÊï∞
SERVER_NAME = os.getenv("GRADIO_SERVER_NAME", "0.0.0.0")
SERVER_PORT = int(os.getenv("GRADIO_SERVER_PORT", "7860"))

with gr.Blocks() as demo:
    MainLayout(
        [
            {
                "name" : "Â≠¶Áøí„ÅÆÈñãÂßã!",
                "component" : TrainingForm
            },
            {
                "name" : "‰∫àÊ∏¨„ÅÆÈñãÂßã!",
                "component" : PredictionForm
            },
            {
                "name" : "‰∫àÊ∏¨ÁµêÊûú„ÅÆÂèñÂæó",
                "component" : SelectableTable,
                "args" : [checkbox_demo_data]
            },
        ]
    )

if __name__ == "__main__":
    logger.info(f"Starting PricingAI Gradio App on {SERVER_NAME}:{SERVER_PORT}")

    demo.launch(
        server_name=SERVER_NAME,
        server_port=SERVER_PORT,
        share=False,
        show_error=True,
        quiet=False
    )
```

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] app.py „Å´Áí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆöËøΩÂä†
- [ ] „É≠„ÇÆ„É≥„Ç∞Ë®≠ÂÆöËøΩÂä†
- [ ] „É≠„Éº„Ç´„É´„ÅßÂãï‰ΩúÁ¢∫Ë™ç (`python app.py`)

---

#### 1.4 .gitignore „ÅÆÊõ¥Êñ∞

**ÊâÄË¶ÅÊôÇÈñì**: 10ÂàÜ

**ËøΩÂä†ÂÜÖÂÆπ**:
```gitignore
# Áí∞Â¢ÉÂ§âÊï∞
.env
.env.local
.env.*.local

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/

# Gradio
gradio_cached_examples/
flagged/

# IDE
.vscode/
.idea/
*.swp

# OS
.DS_Store
Thumbs.db

# Databricks
.databricks/

# Logs
*.log
```

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] .gitignore Êõ¥Êñ∞
- [ ] ‰∏çË¶Å„Å™„Éï„Ç°„Ç§„É´„ÅåÈô§Â§ñ„Åï„Çå„Å¶„ÅÑ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç

---

### Phase 1 ÂÆå‰∫ÜÂü∫Ê∫ñ
- [ ] .env.example „Å® app.yaml „Åå‰ΩúÊàêÊ∏à„Åø
- [ ] app.py „ÅåÁí∞Â¢ÉÂ§âÊï∞„Çí‰ΩøÁî®„Åô„Çã„Çà„ÅÜ„Å´‰øÆÊ≠£Ê∏à„Åø
- [ ] „É≠„Éº„Ç´„É´Áí∞Â¢É„Åß `python app.py` „ÅåÊ≠£Â∏∏„Å´Ëµ∑Âãï
- [ ] .gitignore „ÅåÈÅ©Âàá„Å´Ë®≠ÂÆöÊ∏à„Åø

---

## üìÖ Phase 2: DatabricksÁµ±ÂêàÊ∫ñÂÇô (2-3Êó•)

### ÁõÆÊ®ô
Databricks Jobs API„Å®„ÅÆÁµ±Âêà„ÇíÂÆåÊàê„Åï„Åõ„ÄÅDatabricks Notebook„Çí‰ΩúÊàê„Åô„Çã

### „Çø„Çπ„ÇØ

#### 2.1 Databricks Notebook„ÅÆ‰ΩúÊàê ‚≠ê **ÈáçË¶Å**

**ÊâÄË¶ÅÊôÇÈñì**: 3-4ÊôÇÈñì

**‰ΩúÊàêÂ†¥ÊâÄ**: Databricks Workspace `/Users/{your_email}/PricingAI/`

**„Éï„Ç°„Ç§„É´ÊßãÊàê**:
```
/Users/{your_email}/PricingAI/
‚îú‚îÄ‚îÄ TrainingNotebook.py          # „Éà„É¨„Éº„Éã„É≥„Ç∞„Ç∏„Éß„ÉñÁî®Notebook
‚îú‚îÄ‚îÄ PredictionNotebook.py        # ‰∫àÊ∏¨„Ç∏„Éß„ÉñÁî®Notebook
‚îî‚îÄ‚îÄ Sp_interface.py              # MLÈñ¢Êï∞ (PricingAI_Spark-main „Åã„Çâ„Ç≥„Éî„Éº)
```

**TrainingNotebook.py „ÅÆÂÆüË£Ö**:
```python
# Databricks notebook source
# MAGIC %md
# MAGIC # PricingAI „É¢„Éá„É´„Éà„É¨„Éº„Éã„É≥„Ç∞ Notebook

# COMMAND ----------
# „Éë„É©„É°„Éº„Çø„Ç¶„Ç£„Ç∏„Çß„ÉÉ„Éà„ÅÆ‰ΩúÊàê
dbutils.widgets.text("case_period_start", "2024-01-01")
dbutils.widgets.text("case_period_end", "2024-12-31")
dbutils.widgets.text("stores", "001,002,003")
dbutils.widgets.text("output_zero", "/dbfs/mnt/models/pricing_ai/model_zero")
dbutils.widgets.text("output_up", "/dbfs/mnt/models/pricing_ai/model_up")
dbutils.widgets.text("verbose", "true")

# COMMAND ----------
# „Éë„É©„É°„Éº„Çø„ÅÆÂèñÂæó„Å®Ê§úË®º
case_period_start = dbutils.widgets.get("case_period_start")
case_period_end = dbutils.widgets.get("case_period_end")
stores_str = dbutils.widgets.get("stores")
output_zero = dbutils.widgets.get("output_zero")
output_up = dbutils.widgets.get("output_up")
verbose = dbutils.widgets.get("verbose").lower() == "true"

# „Éê„É™„Éá„Éº„Ç∑„Éß„É≥
from datetime import datetime
try:
    datetime.strptime(case_period_start, "%Y-%m-%d")
    datetime.strptime(case_period_end, "%Y-%m-%d")
except ValueError as e:
    dbutils.notebook.exit(json.dumps({
        "status": "error",
        "message": f"Êó•‰ªò„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Ç®„É©„Éº: {str(e)}"
    }))

# COMMAND ----------
# „Éë„É©„É°„Éº„ÇøÂ§âÊèõ
case_period = (case_period_start, case_period_end)
stores = stores_str.split(",")

print(f"Â≠¶ÁøíÊúüÈñì: {case_period}")
print(f"ÂØæË±°Â∫óËàó: {stores}")
print(f"Verbose: {verbose}")

# COMMAND ----------
# MLÈñ¢Êï∞„ÅÆË™≠„ÅøËæº„Åø
%run ./Sp_interface

# COMMAND ----------
# „É¢„Éá„É´„Éà„É¨„Éº„Éã„É≥„Ç∞ÂÆüË°å
import time
import json

start_time = time.time()

try:
    train_model(
        case_period=case_period,
        stores=stores,
        output_zero=output_zero,
        output_up=output_up,
        verbose=verbose
    )

    elapsed_time = time.time() - start_time

    result = {
        "status": "success",
        "message": "„É¢„Éá„É´„Éà„É¨„Éº„Éã„É≥„Ç∞„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü",
        "elapsed_time": elapsed_time,
        "model_paths": {
            "zero": output_zero,
            "up": output_up
        },
        "parameters": {
            "case_period": case_period,
            "stores": stores
        }
    }

    print("=== „Éà„É¨„Éº„Éã„É≥„Ç∞ÂÆå‰∫Ü ===")
    print(json.dumps(result, indent=2, ensure_ascii=False))

    dbutils.notebook.exit(json.dumps(result, ensure_ascii=False))

except Exception as e:
    import traceback
    error_result = {
        "status": "error",
        "message": str(e),
        "error_type": type(e).__name__,
        "traceback": traceback.format_exc()
    }

    print("=== „Éà„É¨„Éº„Éã„É≥„Ç∞„Ç®„É©„Éº ===")
    print(json.dumps(error_result, indent=2, ensure_ascii=False))

    dbutils.notebook.exit(json.dumps(error_result, ensure_ascii=False))

# COMMAND ----------
```

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] TrainingNotebook.py ‰ΩúÊàê
- [ ] PredictionNotebook.py ‰ΩúÊàê
- [ ] PricingAI_Spark-main „ÅÆ MLÈñ¢Êï∞„Çí Databricks Workspace „Å´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ
- [ ] Notebook Âçò‰Ωì„ÅßÂãï‰ΩúÁ¢∫Ë™ç

---

#### 2.2 Databricks Jobs „ÅÆ‰ΩúÊàê

**ÊâÄË¶ÅÊôÇÈñì**: 2ÊôÇÈñì

**ÊâãÈ†Ü**:

1. **„Éà„É¨„Éº„Éã„É≥„Ç∞„Ç∏„Éß„Éñ‰ΩúÊàê**:
   - Databricks UI: Workflows > Jobs > Create Job
   - Job Name: `PricingAI-Training`
   - Task: Notebook
   - Notebook Path: `/Users/{your_email}/PricingAI/TrainingNotebook`
   - Cluster: Êñ∞Ë¶è„ÇØ„É©„Çπ„Çø„Éº (Spark 3.x, Standard_DS3_v2, 2 workers)

2. **‰∫àÊ∏¨„Ç∏„Éß„Éñ‰ΩúÊàê**:
   - Job Name: `PricingAI-Prediction`
   - Notebook Path: `/Users/{your_email}/PricingAI/PredictionNotebook`

3. **Job ID „ÅÆÂèñÂæó**:
   - ÂêÑ„Ç∏„Éß„Éñ„ÅÆË©≥Á¥∞„Éö„Éº„Ç∏„Åã„ÇâJob ID„ÇíÂèñÂæó
   - .env.local „Å´Ë®≠ÂÆö:
     ```bash
     TRAINING_JOB_ID=933047963643733
     PREDICTION_JOB_ID=933047963643734
     ```

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] „Éà„É¨„Éº„Éã„É≥„Ç∞„Ç∏„Éß„Éñ‰ΩúÊàêÂÆå‰∫Ü
- [ ] ‰∫àÊ∏¨„Ç∏„Éß„Éñ‰ΩúÊàêÂÆå‰∫Ü
- [ ] Job ID „Çí .env.local „Å´Ë®≠ÂÆö
- [ ] ÊâãÂãïÂÆüË°å„ÅßÂãï‰ΩúÁ¢∫Ë™ç

---

#### 2.3 APIÁµ±Âêà„Ç≥„Éº„Éâ„ÅÆÊîπÂñÑ

**ÊâÄË¶ÅÊôÇÈñì**: 2-3ÊôÇÈñì

**‰øÆÊ≠£„Éï„Ç°„Ç§„É´**: `lib/api/apiClient.py`

**ÊîπÂñÑÂÜÖÂÆπ**:
```python
# lib/api/apiClient.py (ÊîπÂñÑÁâà)
import requests
import os
import logging
import time
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

class DatabricksJobsClient:
    """Databricks Jobs API „ÇØ„É©„Ç§„Ç¢„É≥„Éà"""

    def __init__(self):
        self.host = os.getenv("DATABRICKS_HOST", "").rstrip("/")
        self.token = os.getenv("DATABRICKS_TOKEN")

        if not self.host or not self.token:
            raise ValueError("DATABRICKS_HOST „Å® DATABRICKS_TOKEN „ÇíË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ")

        self.headers = {
            "Authorization": f"Bearer {self.token}",
            "Content-Type": "application/json"
        }

    def run_job(self, job_id: int, notebook_params: Dict[str, str]) -> Dict[str, Any]:
        """
        Databricks Job„ÇíÂÆüË°å

        Parameters:
        -----------
        job_id : int
            ÂÆüË°å„Åô„Çã„Ç∏„Éß„Éñ„ÅÆID
        notebook_params : Dict[str, str]
            Notebook„Å´Ê∏°„Åô„Éë„É©„É°„Éº„Çø

        Returns:
        --------
        Dict[str, Any]
            run_id „ÇíÂê´„ÇÄ„É¨„Çπ„Éù„É≥„Çπ
        """
        url = f"{self.host}/api/2.2/jobs/run-now"
        payload = {
            "job_id": job_id,
            "notebook_params": notebook_params
        }

        logger.info(f"Running job {job_id} with params: {notebook_params}")

        try:
            response = requests.post(url, headers=self.headers, json=payload)
            response.raise_for_status()

            result = response.json()
            logger.info(f"Job started successfully. Run ID: {result.get('run_id')}")
            return result

        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to run job: {str(e)}")
            raise

    def get_run_status(self, run_id: int) -> Dict[str, Any]:
        """
        „Ç∏„Éß„ÉñÂÆüË°å„ÅÆ„Çπ„ÉÜ„Éº„Çø„Çπ„ÇíÂèñÂæó

        Parameters:
        -----------
        run_id : int
            ÂÆüË°åID

        Returns:
        --------
        Dict[str, Any]
            ÂÆüË°å„Çπ„ÉÜ„Éº„Çø„ÇπÊÉÖÂ†±
        """
        url = f"{self.host}/api/2.2/jobs/runs/get"
        params = {"run_id": run_id}

        try:
            response = requests.get(url, headers=self.headers, params=params)
            response.raise_for_status()
            return response.json()

        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to get run status: {str(e)}")
            raise

    def get_run_output(self, run_id: int) -> Optional[str]:
        """
        „Ç∏„Éß„ÉñÂÆüË°å„ÅÆÂá∫ÂäõÁµêÊûú„ÇíÂèñÂæó

        Parameters:
        -----------
        run_id : int
            ÂÆüË°åID (task run_id)

        Returns:
        --------
        Optional[str]
            ÂÆüË°åÁµêÊûú (JSONÊñáÂ≠óÂàó)
        """
        url = f"{self.host}/api/2.2/jobs/runs/get-output"
        params = {"run_id": run_id}

        try:
            response = requests.get(url, headers=self.headers, params=params)
            response.raise_for_status()

            result = response.json()
            return result.get("notebook_output", {}).get("result")

        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to get run output: {str(e)}")
            raise

    def wait_for_completion(self, run_id: int, max_wait: int = 3600,
                           check_interval: int = 10) -> Dict[str, Any]:
        """
        „Ç∏„Éß„ÉñÂÆüË°å„ÅÆÂÆå‰∫Ü„ÇíÂæÖÊ©ü

        Parameters:
        -----------
        run_id : int
            ÂÆüË°åID
        max_wait : int
            ÊúÄÂ§ßÂæÖÊ©üÊôÇÈñìÔºàÁßíÔºâ
        check_interval : int
            „ÉÅ„Çß„ÉÉ„ÇØÈñìÈöîÔºàÁßíÔºâ

        Returns:
        --------
        Dict[str, Any]
            ÊúÄÁµÇÁöÑ„Å™ÂÆüË°å„Çπ„ÉÜ„Éº„Çø„Çπ
        """
        start_time = time.time()

        while time.time() - start_time < max_wait:
            status = self.get_run_status(run_id)
            state = status.get("state", {})
            life_cycle_state = state.get("life_cycle_state")

            logger.info(f"Run {run_id} status: {life_cycle_state}")

            if life_cycle_state in ["TERMINATED", "SKIPPED", "INTERNAL_ERROR"]:
                return status

            time.sleep(check_interval)

        raise TimeoutError(f"Job execution timed out after {max_wait} seconds")

# „Ç∞„É≠„Éº„Éê„É´„ÇØ„É©„Ç§„Ç¢„É≥„Éà„Ç§„É≥„Çπ„Çø„É≥„Çπ
_client = None

def get_databricks_client() -> DatabricksJobsClient:
    """Databricks Jobs API„ÇØ„É©„Ç§„Ç¢„É≥„Éà„ÅÆ„Ç∑„É≥„Ç∞„É´„Éà„É≥„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÇíÂèñÂæó"""
    global _client
    if _client is None:
        _client = DatabricksJobsClient()
    return _client


# ÂæåÊñπ‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅÊÆã„Åô
def apiClient(payload):
    """
    ÊóßAPI„ÇØ„É©„Ç§„Ç¢„É≥„Éà (ÂæåÊñπ‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅÊÆã„Åô)

    Êñ∞„Åó„ÅÑ„Ç≥„Éº„Éâ„Åß„ÅØ get_databricks_client() „Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ
    """
    logger.warning("apiClient() „ÅØÈùûÊé®Â•®„Åß„Åô„ÄÇget_databricks_client() „Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
    # Á∞°ÊòìÁöÑ„Å™ÂÆüË£Ö
    return {"status": "deprecated"}
```

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] apiClient.py „ÇíÊîπÂñÑÁâà„Å´Êõ¥Êñ∞
- [ ] Áí∞Â¢ÉÂ§âÊï∞ DATABRICKS_HOST „Çí .env.example „Å´ËøΩÂä†
- [ ] „É¶„Éã„ÉÉ„Éà„ÉÜ„Çπ„Éà„Çí‰ΩúÊàêÔºàÂæå„ÅßÂèØÔºâ

---

#### 2.4 „Éï„Ç©„Éº„É†ÈÄÅ‰ø°Âá¶ÁêÜ„ÅÆÊõ¥Êñ∞

**ÊâÄË¶ÅÊôÇÈñì**: 1-2ÊôÇÈñì

**‰øÆÊ≠£„Éï„Ç°„Ç§„É´**: `lib/api/sending.py`

**ÊîπÂñÑÂÜÖÂÆπ**:
```python
# lib/api/sending.py (ÊîπÂñÑÁâà)
from settings import *
from lib.utils.data_utils import to_datestr
from lib.api.apiClient import get_databricks_client
import logging
import os
import json

logger = logging.getLogger(__name__)

def send_data(selected_stores, start_date, end_date):
    """
    Â≠¶Áøí„Ç∏„Éß„Éñ„ÇíÈñãÂßã

    Parameters:
    -----------
    selected_stores : list[str]
        ÈÅ∏Êäû„Åï„Çå„ÅüÂ∫óËàóÂêç„ÅÆ„É™„Çπ„Éà
    start_date : str
        ÈñãÂßãÊó•
    end_date : str
        ÁµÇ‰∫ÜÊó•

    Returns:
    --------
    str
        ÂÆüË°åÁµêÊûú„É°„ÉÉ„Çª„Éº„Ç∏
    """
    # „Éê„É™„Éá„Éº„Ç∑„Éß„É≥
    if not selected_stores or not start_date or not end_date:
        return "‚ùå Â∫óËàó„ÉªÊúüÈñì„Çí„Åô„Åπ„Å¶ÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"

    try:
        # Êó•‰ªòÂ§âÊèõ
        start_date_str = to_datestr(start_date)
        end_date_str = to_datestr(end_date)

        # Â∫óËàó„Ç≥„Éº„ÉâÂèñÂæó
        store_codes = DF_STORES.loc[
            DF_STORES["store_name"].isin(selected_stores),
            "store_code"
        ].tolist()
        store_codes_str = ",".join([str(code) for code in store_codes])

        # Job „Éë„É©„É°„Éº„Çø‰ΩúÊàê
        notebook_params = {
            "case_period_start": start_date_str,
            "case_period_end": end_date_str,
            "stores": store_codes_str,
            "output_zero": "/dbfs/mnt/models/pricing_ai/model_zero",
            "output_up": "/dbfs/mnt/models/pricing_ai/model_up",
            "verbose": "true"
        }

        # Databricks Jobs APIÂÆüË°å
        client = get_databricks_client()
        training_job_id = int(os.getenv("TRAINING_JOB_ID"))

        logger.info(f"Starting training job {training_job_id}")
        result = client.run_job(training_job_id, notebook_params)

        run_id = result.get("run_id")

        return f"""‚úÖ Â≠¶Áøí„Ç∏„Éß„Éñ„ÇíÈñãÂßã„Åó„Åæ„Åó„Åü

üÜî Run ID: {run_id}
üìÖ Â≠¶ÁøíÊúüÈñì: {start_date_str} ~ {end_date_str}
üè™ ÂØæË±°Â∫óËàó: {len(store_codes)}Â∫óËàó

„Ç∏„Éß„Éñ„ÅÆÈÄ≤Ë°åÁä∂Ê≥Å„ÅØ Databricks UI „ÅßÁ¢∫Ë™ç„Åß„Åç„Åæ„Åô„ÄÇ
"""

    except Exception as e:
        logger.error(f"Failed to start training job: {str(e)}")
        return f"‚ùå „Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}"


def send_pre(pred_store_name, pred_date):
    """
    ‰∫àÊ∏¨„Ç∏„Éß„Éñ„ÇíÈñãÂßã

    Parameters:
    -----------
    pred_store_name : str
        ‰∫àÊ∏¨ÂØæË±°Â∫óËàóÂêç
    pred_date : str
        ‰∫àÊ∏¨Êó•

    Returns:
    --------
    str
        ÂÆüË°åÁµêÊûú„É°„ÉÉ„Çª„Éº„Ç∏
    """
    # „Éê„É™„Éá„Éº„Ç∑„Éß„É≥
    if not pred_store_name or not pred_date:
        return "‚ùå Â∫óËàóÂêç„Å®‰∫àÊ∏¨Êó•„Çí‰∏°ÊñπÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ"

    try:
        pred_date_str = to_datestr(pred_date)
        store_code_str = str(
            DF_STORES.loc[DF_STORES["store_name"] == pred_store_name, "store_code"].iat[0]
        )

        # Job „Éë„É©„É°„Éº„Çø‰ΩúÊàê
        notebook_params = {
            "store_cd": store_code_str,
            "upday": pred_date_str,
            "model_zero": "/dbfs/mnt/models/pricing_ai/model_zero",
            "model_up": "/dbfs/mnt/models/pricing_ai/model_up",
            "item_file": "/dbfs/mnt/data/item_master.csv",
            "drop_file": "/dbfs/mnt/data/drop_items.csv",
            "verbose": "true"
        }

        # Databricks Jobs APIÂÆüË°å
        client = get_databricks_client()
        prediction_job_id = int(os.getenv("PREDICTION_JOB_ID"))

        logger.info(f"Starting prediction job {prediction_job_id}")
        result = client.run_job(prediction_job_id, notebook_params)

        run_id = result.get("run_id")

        return f"""‚úÖ ‰∫àÊ∏¨„Ç∏„Éß„Éñ„ÇíÈñãÂßã„Åó„Åæ„Åó„Åü

üÜî Run ID: {run_id}
üè™ ÂØæË±°Â∫óËàó: {pred_store_name} ({store_code_str})
üìÖ ‰∫àÊ∏¨Êó•: {pred_date_str}

„Ç∏„Éß„Éñ„ÅÆÈÄ≤Ë°åÁä∂Ê≥Å„ÅØ Databricks UI „ÅßÁ¢∫Ë™ç„Åß„Åç„Åæ„Åô„ÄÇ
"""

    except Exception as e:
        logger.error(f"Failed to start prediction job: {str(e)}")
        return f"‚ùå „Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}"
```

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] sending.py „ÇíÊîπÂñÑÁâà„Å´Êõ¥Êñ∞
- [ ] TrainingForm.py, PredictionForm.py „ÅßÂãï‰ΩúÁ¢∫Ë™ç
- [ ] „Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞„ÅÆÁ¢∫Ë™ç

---

#### 2.5 Secret Scope „ÅÆ‰ΩúÊàê

**ÊâÄË¶ÅÊôÇÈñì**: 30ÂàÜ

**ÊâãÈ†Ü**:

1. Databricks UI „Åß Secret Scope ‰ΩúÊàê:
   - Settings > Developer > Secret Scopes
   - Scope Name: `pricing-ai-secrets`
   - Manage Principal: Creator

2. Secret „ÅÆÁôªÈå≤:
   ```bash
   databricks secrets put-secret \
     --scope pricing-ai-secrets \
     --key databricks-token
   # „Éó„É≠„É≥„Éó„Éà„Åß„Éà„Éº„ÇØ„É≥„ÇíÂÖ•Âäõ
   ```

3. app.yaml „Å´„Ç∑„Éº„ÇØ„É¨„ÉÉ„ÉàË®≠ÂÆöËøΩÂä†:
   ```yaml
   secrets:
     - scope: pricing-ai-secrets
       key: databricks-token
       env_var: DATABRICKS_TOKEN
   ```

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] Secret Scope‰ΩúÊàêÂÆå‰∫Ü
- [ ] databricks-token „Ç∑„Éº„ÇØ„É¨„ÉÉ„ÉàÁôªÈå≤ÂÆå‰∫Ü
- [ ] app.yaml „Å´„Ç∑„Éº„ÇØ„É¨„ÉÉ„ÉàË®≠ÂÆöËøΩÂä†

---

### Phase 2 ÂÆå‰∫ÜÂü∫Ê∫ñ
- [ ] Databricks Notebook‰ΩúÊàêÂÆå‰∫Ü
- [ ] Databricks Jobs‰ΩúÊàêÂÆå‰∫Ü
- [ ] APIÁµ±Âêà„Ç≥„Éº„ÉâÊîπÂñÑÂÆå‰∫Ü
- [ ] Secret ScopeË®≠ÂÆöÂÆå‰∫Ü
- [ ] „É≠„Éº„Ç´„É´‚ÜíDatabricks Jobs „ÅÆÈÄö‰ø°Á¢∫Ë™çÂÆå‰∫Ü

---

## üìÖ Phase 3: Databricks Apps „Éá„Éó„É≠„Ç§ (3-5Êó•)

### ÁõÆÊ®ô
Databricks Apps„Å´Gradio UI„Çí„Éá„Éó„É≠„Ç§„Åó„ÄÅÂãï‰ΩúÁ¢∫Ë™ç„Åô„Çã

### „Çø„Çπ„ÇØ

#### 3.1 Databricks App „ÅÆ‰ΩúÊàê

**ÊâÄË¶ÅÊôÇÈñì**: 1ÊôÇÈñì

**ÊâãÈ†Ü** (CLI‰ΩøÁî®):
```bash
# 1. Databricks CLI „Ç§„É≥„Çπ„Éà„Éº„É´
pip install databricks-cli

# 2. Ë™çË®ºË®≠ÂÆö
databricks configure --token
# Host: https://adb-xxxxx.azuredatabricks.net
# Token: [your access token]

# 3. App ‰ΩúÊàê
databricks apps create \
  --app-name pricing-ai-frontend-development \
  --git-url https://github.com/igawa-nac/GRADIO-CLEANUP.git \
  --git-branch develop \
  --git-path "PricingAIFrontend-develop 2" \
  --description "PricingAI Gradio UI - ÈñãÁô∫Áí∞Â¢É"
```

**„Åæ„Åü„ÅØ UI „Åã„Çâ**:
1. Apps > Create App
2. Ë®≠ÂÆö:
   - App Name: `pricing-ai-frontend-development`
   - Source: Git Repository
   - Repository: `https://github.com/igawa-nac/GRADIO-CLEANUP.git`
   - Branch: `develop`
   - Path: `PricingAIFrontend-develop 2`

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] App‰ΩúÊàêÂÆå‰∫Ü
- [ ] GitÈÄ£Êê∫Ë®≠ÂÆöÂÆå‰∫Ü
- [ ] App URL„ÅÆÂèñÂæó

---

#### 3.2 ÂàùÂõû„Éá„Éó„É≠„Ç§„Å®„Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞

**ÊâÄË¶ÅÊôÇÈñì**: 2-3ÊôÇÈñì

**„Éá„Éó„É≠„Ç§ÂÆüË°å**:
```bash
databricks apps deploy --app-name pricing-ai-frontend-development
```

**ÊÉ≥ÂÆö„Åï„Çå„ÇãÂïèÈ°å„Å®ÂØæÂá¶**:

| ÂïèÈ°å | ÂéüÂõ† | Ëß£Ê±∫Á≠ñ |
|------|------|--------|
| ModuleNotFoundError | requirements.txt ‰∏çË∂≥ | ‰æùÂ≠òÈñ¢‰øÇ„ÇíËøΩÂä† |
| Port binding error | „Éù„Éº„ÉàË®≠ÂÆö„Éü„Çπ | app.yaml „ÅÆ„Éù„Éº„ÉàÁ¢∫Ë™ç |
| Secret not found | „Ç∑„Éº„ÇØ„É¨„ÉÉ„ÉàÊú™Ë®≠ÂÆö | Secret ScopeÁ¢∫Ë™ç |
| Permission denied | „Ç¢„ÇØ„Çª„ÇπÊ®©Èôê‰∏çË∂≥ | WorkspaceÁÆ°ÁêÜËÄÖ„Å´ÈÄ£Áµ° |

**„É≠„Ç∞Á¢∫Ë™ç**:
```bash
# „É≠„Ç∞„ÅÆÁ¢∫Ë™ç
databricks apps logs --app-name pricing-ai-frontend-development --tail 100

# „É™„Ç¢„É´„Çø„Ç§„É†„É≠„Ç∞
databricks apps logs --app-name pricing-ai-frontend-development --follow
```

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] „Éá„Éó„É≠„Ç§ÊàêÂäü
- [ ] „Ç¢„Éó„É™„ÅåËµ∑Âãï
- [ ] UI„Å´„Ç¢„ÇØ„Çª„ÇπÂèØËÉΩ
- [ ] „Ç®„É©„Éº„É≠„Ç∞„Åå„Å™„ÅÑ„Åì„Å®„ÇíÁ¢∫Ë™ç

---

#### 3.3 Âãï‰ΩúÁ¢∫Ë™ç„Å®„Éá„Éê„ÉÉ„Ç∞

**ÊâÄË¶ÅÊôÇÈñì**: 2-3ÊôÇÈñì

**Á¢∫Ë™çÈ†ÖÁõÆ**:

1. **UIË°®Á§∫Á¢∫Ë™ç**:
   - [ ] „Éà„É¨„Éº„Éã„É≥„Ç∞„Éï„Ç©„Éº„É†„ÅåÊ≠£Â∏∏„Å´Ë°®Á§∫
   - [ ] ‰∫àÊ∏¨„Éï„Ç©„Éº„É†„ÅåÊ≠£Â∏∏„Å´Ë°®Á§∫
   - [ ] „ÉÜ„Éº„Éñ„É´„ÅåÊ≠£Â∏∏„Å´Ë°®Á§∫

2. **Ê©üËÉΩÁ¢∫Ë™ç**:
   - [ ] Â∫óËàóÈÅ∏Êäû„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥„ÅåÂãï‰Ωú
   - [ ] Êó•‰ªò„Éî„ÉÉ„Ç´„Éº„ÅåÂãï‰Ωú
   - [ ] Â≠¶ÁøíÈñãÂßã„Éú„Çø„É≥„ÇØ„É™„ÉÉ„ÇØ ‚Üí Run IDË°®Á§∫
   - [ ] Databricks Jobs„ÅåÂÆüÈöõ„Å´Ëµ∑Âãï

3. **„Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞Á¢∫Ë™ç**:
   - [ ] ÂøÖÈ†àÈ†ÖÁõÆÊú™ÂÖ•ÂäõÊôÇ„ÅÆ„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏
   - [ ] APIÈÄö‰ø°„Ç®„É©„ÉºÊôÇ„ÅÆÈÅ©Âàá„Å™„É°„ÉÉ„Çª„Éº„Ç∏

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] „Åô„Åπ„Å¶„ÅÆÊ©üËÉΩ„ÅåÊ≠£Â∏∏Âãï‰Ωú
- [ ] „Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞„ÅåÈÅ©Âàá
- [ ] „É≠„Ç∞„Å´Áï∞Â∏∏„Åå„Å™„ÅÑ

---

### Phase 3 ÂÆå‰∫ÜÂü∫Ê∫ñ
- [ ] Databricks Apps „Éá„Éó„É≠„Ç§ÂÆå‰∫Ü
- [ ] UI „ÅåÊ≠£Â∏∏„Å´Âãï‰Ωú
- [ ] Databricks Jobs „Å®„ÅÆÈÄ£Êê∫Á¢∫Ë™çÂÆå‰∫Ü
- [ ] Âü∫Êú¨ÁöÑ„Å™Âãï‰ΩúÁ¢∫Ë™çÂÆå‰∫Ü

---

## üìÖ Phase 4: Êú¨Áï™Áµ±Âêà„Å®„ÉÜ„Çπ„Éà (5-7Êó•)

### ÁõÆÊ®ô
Êú¨Áï™Áí∞Â¢É„Å∏„ÅÆÊ∫ñÂÇô„Å®„ÄÅÂåÖÊã¨ÁöÑ„Å™„ÉÜ„Çπ„Éà„ÅÆÂÆüÊñΩ

### „Çø„Çπ„ÇØ

#### 4.1 CI/CD„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆÊßãÁØâ

**ÊâÄË¶ÅÊôÇÈñì**: 3-4ÊôÇÈñì

**„Éï„Ç°„Ç§„É´„Éë„Çπ**: `.github/workflows/deploy-databricks-apps.yml`

**ÂÆüË£ÖÂÜÖÂÆπ**:
```yaml
name: Deploy to Databricks Apps

on:
  push:
    branches:
      - main
      - develop

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        working-directory: ./PricingAIFrontend-develop 2
        run: |
          pip install -r requirements.txt
          pip install pytest ruff

      - name: Lint with ruff
        working-directory: ./PricingAIFrontend-develop 2
        run: |
          ruff check .

      - name: Run tests
        working-directory: ./PricingAIFrontend-develop 2
        run: |
          pytest tests/ || echo "No tests found"

      - name: Install Databricks CLI
        run: |
          pip install databricks-cli

      - name: Configure Databricks CLI
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          echo "[DEFAULT]" > ~/.databrickscfg
          echo "host = $DATABRICKS_HOST" >> ~/.databrickscfg
          echo "token = $DATABRICKS_TOKEN" >> ~/.databrickscfg

      - name: Determine environment
        id: env
        run: |
          if [ "${{ github.ref }}" == "refs/heads/main" ]; then
            echo "app_name=pricing-ai-frontend-production" >> $GITHUB_OUTPUT
            echo "environment=production" >> $GITHUB_OUTPUT
          else
            echo "app_name=pricing-ai-frontend-development" >> $GITHUB_OUTPUT
            echo "environment=development" >> $GITHUB_OUTPUT
          fi

      - name: Deploy to Databricks Apps
        run: |
          databricks apps deploy \
            --app-name ${{ steps.env.outputs.app_name }} \
            --git-branch ${{ github.ref_name }}

      - name: Verify deployment
        run: |
          databricks apps get --app-name ${{ steps.env.outputs.app_name }}

      - name: Notify deployment
        if: success()
        run: |
          echo "Deployment to ${{ steps.env.outputs.environment }} successful!"
```

**GitHub Secrets Ë®≠ÂÆö**:
- `DATABRICKS_HOST`: Databricks „ÉØ„Éº„ÇØ„Çπ„Éö„Éº„ÇπURL
- `DATABRICKS_TOKEN`: Databricks„Ç¢„ÇØ„Çª„Çπ„Éà„Éº„ÇØ„É≥

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] GitHub Actions „ÉØ„Éº„ÇØ„Éï„É≠„Éº‰ΩúÊàê
- [ ] GitHub Secrets Ë®≠ÂÆö
- [ ] develop „Éñ„É©„É≥„ÉÅ„Å∏„ÅÆpush„ÅßËá™Âãï„Éá„Éó„É≠„Ç§Á¢∫Ë™ç
- [ ] main „Éñ„É©„É≥„ÉÅ„Å∏„ÅÆpush„ÅßËá™Âãï„Éá„Éó„É≠„Ç§Á¢∫Ë™ç

---

#### 4.2 „ÉÜ„Çπ„Éà„Ç≥„Éº„Éâ„ÅÆ‰ΩúÊàê

**ÊâÄË¶ÅÊôÇÈñì**: 4-6ÊôÇÈñì

**„Éá„Ç£„É¨„ÇØ„Éà„É™ÊßãÈÄ†**:
```
PricingAIFrontend-develop 2/
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ unit/
    ‚îÇ   ‚îú‚îÄ‚îÄ test_api_client.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_sending.py
    ‚îÇ   ‚îî‚îÄ‚îÄ test_data_utils.py
    ‚îú‚îÄ‚îÄ integration/
    ‚îÇ   ‚îú‚îÄ‚îÄ test_training_flow.py
    ‚îÇ   ‚îî‚îÄ‚îÄ test_prediction_flow.py
    ‚îî‚îÄ‚îÄ conftest.py
```

**„Çµ„É≥„Éó„É´„ÉÜ„Çπ„Éà„Ç≥„Éº„Éâ**:
```python
# tests/unit/test_api_client.py
import pytest
from unittest.mock import Mock, patch
from lib.api.apiClient import DatabricksJobsClient

@pytest.fixture
def mock_env(monkeypatch):
    """Áí∞Â¢ÉÂ§âÊï∞„ÅÆ„É¢„ÉÉ„ÇØ"""
    monkeypatch.setenv("DATABRICKS_HOST", "https://test.azuredatabricks.net")
    monkeypatch.setenv("DATABRICKS_TOKEN", "test-token")

def test_databricks_client_initialization(mock_env):
    """„ÇØ„É©„Ç§„Ç¢„É≥„ÉàÂàùÊúüÂåñ„ÅÆ„ÉÜ„Çπ„Éà"""
    client = DatabricksJobsClient()
    assert client.host == "https://test.azuredatabricks.net"
    assert client.token == "test-token"

@patch('requests.post')
def test_run_job_success(mock_post, mock_env):
    """„Ç∏„Éß„ÉñÂÆüË°åÊàêÂäü„ÅÆ„ÉÜ„Çπ„Éà"""
    # „É¢„ÉÉ„ÇØ„É¨„Çπ„Éù„É≥„Çπ
    mock_response = Mock()
    mock_response.json.return_value = {"run_id": 12345}
    mock_response.status_code = 200
    mock_post.return_value = mock_response

    client = DatabricksJobsClient()
    result = client.run_job(
        job_id=123,
        notebook_params={"test": "param"}
    )

    assert result["run_id"] == 12345
    mock_post.assert_called_once()
```

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] „É¶„Éã„ÉÉ„Éà„ÉÜ„Çπ„Éà‰ΩúÊàê
- [ ] Áµ±Âêà„ÉÜ„Çπ„Éà‰ΩúÊàê
- [ ] „ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏ > 70%
- [ ] „Åô„Åπ„Å¶„ÅÆ„ÉÜ„Çπ„Éà„ÅåÂêàÊ†º

---

#### 4.3 Êú¨Áï™Áí∞Â¢ÉÁî®App„ÅÆ‰ΩúÊàê

**ÊâÄË¶ÅÊôÇÈñì**: 1ÊôÇÈñì

**ÊâãÈ†Ü**:
```bash
databricks apps create \
  --app-name pricing-ai-frontend-production \
  --git-url https://github.com/igawa-nac/GRADIO-CLEANUP.git \
  --git-branch main \
  --git-path "PricingAIFrontend-develop 2" \
  --description "PricingAI Gradio UI - Êú¨Áï™Áí∞Â¢É"
```

**Êú¨Áï™Áí∞Â¢ÉÁî®Ë®≠ÂÆö**:
- „Ç¢„ÇØ„Çª„ÇπÂà∂Èôê„ÅÆË®≠ÂÆö
- „É≠„Ç∞„É¨„Éô„É´„ÅÆË™øÊï¥
- „É™„ÇΩ„Éº„ÇπË®≠ÂÆö„ÅÆÊúÄÈÅ©Âåñ

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] Êú¨Áï™App‰ΩúÊàêÂÆå‰∫Ü
- [ ] „Ç¢„ÇØ„Çª„ÇπÂà∂ÈôêË®≠ÂÆö
- [ ] Êú¨Áï™Áî®Secret ScopeË®≠ÂÆö
- [ ] Êú¨Áï™Áî®Jobs‰ΩúÊàê

---

#### 4.4 „Éâ„Ç≠„É•„É°„É≥„ÉàÊúÄÁµÇÊõ¥Êñ∞

**ÊâÄË¶ÅÊôÇÈñì**: 2ÊôÇÈñì

**Êõ¥Êñ∞„Éâ„Ç≠„É•„É°„É≥„Éà**:
- [ ] README.md: ÂÆüÈöõ„ÅÆ„Éá„Éó„É≠„Ç§ÊâãÈ†Ü„ÇíÂèçÊò†
- [ ] STRATEGY.md: ÊúÄÊñ∞„ÅÆÊÉÖÂ†±„Å´Êõ¥Êñ∞
- [ ] DatabricksAppsSetup.md: ÂÆüÈöõ„ÅÆÊßãÁØâÊâãÈ†Ü„ÇíËøΩÂä†

---

### Phase 4 ÂÆå‰∫ÜÂü∫Ê∫ñ
- [ ] CI/CD„Éë„Ç§„Éó„É©„Ç§„É≥ÊßãÁØâÂÆå‰∫Ü
- [ ] „ÉÜ„Çπ„Éà„Ç≥„Éº„Éâ‰ΩúÊàê„ÉªÂÆüË°åÂÆå‰∫Ü
- [ ] Êú¨Áï™Áí∞Â¢ÉApp‰ΩúÊàêÂÆå‰∫Ü
- [ ] „Éâ„Ç≠„É•„É°„É≥„ÉàÊõ¥Êñ∞ÂÆå‰∫Ü

---

## üìÖ Phase 5: Êú¨Áï™„É™„É™„Éº„Çπ (1Êó•)

### ÁõÆÊ®ô
Êú¨Áï™Áí∞Â¢É„Å∏„ÅÆ„É™„É™„Éº„Çπ„Å®Áõ£Ë¶ñ‰ΩìÂà∂„ÅÆÁ¢∫Á´ã

### „Çø„Çπ„ÇØ

#### 5.1 Êú¨Áï™„É™„É™„Éº„Çπ

**ÊâÄË¶ÅÊôÇÈñì**: 2-3ÊôÇÈñì

**ÊâãÈ†Ü**:
1. develop „Éñ„É©„É≥„ÉÅ„ÅÆÊúÄÁµÇÁ¢∫Ë™ç
2. release „Éñ„É©„É≥„ÉÅ‰ΩúÊàê
3. „É™„É™„Éº„Çπ„Éé„Éº„Éà‰ΩúÊàê
4. main „Éñ„É©„É≥„ÉÅ„Å∏„Éû„Éº„Ç∏
5. „Çø„Ç∞‰ªò‰∏é (v1.0.0)
6. Ëá™Âãï„Éá„Éó„É≠„Ç§Á¢∫Ë™ç

**„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà**:
- [ ] release „Éñ„É©„É≥„ÉÅ‰ΩúÊàê
- [ ] „É™„É™„Éº„Çπ„Éé„Éº„Éà‰ΩúÊàê
- [ ] main „Å∏„Éû„Éº„Ç∏
- [ ] „Çø„Ç∞‰ªò‰∏é„Éª„Éó„ÉÉ„Ç∑„É•
- [ ] Êú¨Áï™Áí∞Â¢É„Éá„Éó„É≠„Ç§Á¢∫Ë™ç

---

#### 5.2 Áõ£Ë¶ñ„Å®„Ç¢„É©„Éº„ÉàË®≠ÂÆö

**ÊâÄË¶ÅÊôÇÈñì**: 1-2ÊôÇÈñì

**Ë®≠ÂÆöÈ†ÖÁõÆ**:
- [ ] „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„É≠„Ç∞„ÅÆÂÆöÊúüÁ¢∫Ë™ç
- [ ] „Ç®„É©„ÉºÁéá„ÅÆÁõ£Ë¶ñ
- [ ] „É¨„Çπ„Éù„É≥„ÇπÊôÇÈñì„ÅÆÁõ£Ë¶ñ
- [ ] DBUÊ∂àË≤ªÈáè„ÅÆÁõ£Ë¶ñ

---

#### 5.3 ÈÅãÁî®„Éâ„Ç≠„É•„É°„É≥„Éà‰ΩúÊàê

**ÊâÄË¶ÅÊôÇÈñì**: 1-2ÊôÇÈñì

**‰ΩúÊàê„Éâ„Ç≠„É•„É°„É≥„Éà**:
- [ ] ÈÅãÁî®ÊâãÈ†ÜÊõ∏
- [ ] „Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞„Ç¨„Ç§„Éâ
- [ ] „É≠„Éº„É´„Éê„ÉÉ„ÇØÊâãÈ†Ü

---

### Phase 5 ÂÆå‰∫ÜÂü∫Ê∫ñ
- [ ] Êú¨Áï™Áí∞Â¢É„É™„É™„Éº„ÇπÂÆå‰∫Ü
- [ ] Áõ£Ë¶ñ‰ΩìÂà∂Á¢∫Á´ã
- [ ] ÈÅãÁî®„Éâ„Ç≠„É•„É°„É≥„Éà‰ΩúÊàêÂÆå‰∫Ü

---

## üìã ÂÖ®‰Ωì„ÉÅ„Çß„ÉÉ„ÇØ„É™„Çπ„Éà

### Áí∞Â¢ÉÊßãÁØâ
- [ ] .env.example ‰ΩúÊàê
- [ ] app.yaml ‰ΩúÊàê
- [ ] app.py Databricks Apps ÂØæÂøú
- [ ] .gitignore Êõ¥Êñ∞

### DatabricksÁµ±Âêà
- [ ] Databricks Notebook ‰ΩúÊàê
- [ ] Databricks Jobs ‰ΩúÊàê
- [ ] Secret Scope ‰ΩúÊàê
- [ ] APIÁµ±Âêà„Ç≥„Éº„ÉâÊîπÂñÑ

### „Éá„Éó„É≠„Ç§
- [ ] ÈñãÁô∫Áí∞Â¢ÉApp‰ΩúÊàê
- [ ] ÂàùÂõû„Éá„Éó„É≠„Ç§ÊàêÂäü
- [ ] Âãï‰ΩúÁ¢∫Ë™çÂÆå‰∫Ü
- [ ] Êú¨Áï™Áí∞Â¢ÉApp‰ΩúÊàê

### „ÉÜ„Çπ„Éà„ÉªÂìÅË≥™‰øùË®º
- [ ] CI/CD„Éë„Ç§„Éó„É©„Ç§„É≥ÊßãÁØâ
- [ ] „ÉÜ„Çπ„Éà„Ç≥„Éº„Éâ‰ΩúÊàê
- [ ] „ÉÜ„Çπ„ÉàÂÆüË°å„ÉªÂêàÊ†º
- [ ] „Éâ„Ç≠„É•„É°„É≥„ÉàÊõ¥Êñ∞

### Êú¨Áï™„É™„É™„Éº„Çπ
- [ ] „É™„É™„Éº„Çπ„Éñ„É©„É≥„ÉÅ‰ΩúÊàê
- [ ] Êú¨Áï™„Éá„Éó„É≠„Ç§
- [ ] Áõ£Ë¶ñË®≠ÂÆö
- [ ] ÈÅãÁî®„Éâ„Ç≠„É•„É°„É≥„Éà‰ΩúÊàê

---

## üéØ ÂÑ™ÂÖàÈ†Ü‰Ωç„Åæ„Å®„ÇÅ

### ÊúÄÂÑ™ÂÖà (‰ªä„Åô„ÅêÁùÄÊâã)
1. **.env.example „Å® app.yaml „ÅÆ‰ΩúÊàê** (Phase 1.1, 1.2)
2. **Databricks Notebook „ÅÆ‰ΩúÊàê** (Phase 2.1)
3. **Databricks Jobs „ÅÆ‰ΩúÊàê** (Phase 2.2)

### È´òÂÑ™ÂÖà (1ÈÄ±Èñì‰ª•ÂÜÖ)
4. **APIÁµ±Âêà„Ç≥„Éº„Éâ„ÅÆÊîπÂñÑ** (Phase 2.3, 2.4)
5. **Secret Scope „ÅÆË®≠ÂÆö** (Phase 2.5)
6. **Databricks Apps „Éá„Éó„É≠„Ç§** (Phase 3.1-3.3)

### ‰∏≠ÂÑ™ÂÖà (2ÈÄ±Èñì‰ª•ÂÜÖ)
7. **CI/CD„Éë„Ç§„Éó„É©„Ç§„É≥ÊßãÁØâ** (Phase 4.1)
8. **„ÉÜ„Çπ„Éà„Ç≥„Éº„Éâ‰ΩúÊàê** (Phase 4.2)
9. **Êú¨Áï™Áí∞Â¢ÉÊ∫ñÂÇô** (Phase 4.3)

### ‰ΩéÂÑ™ÂÖà (3ÈÄ±Èñì‰ª•ÂÜÖ)
10. **Êú¨Áï™„É™„É™„Éº„Çπ** (Phase 5)

---

## üìû „Çµ„Éù„Éº„Éà

Âõ∞„Å£„Åü„Å®„Åç„ÅØ:
1. **„Éâ„Ç≠„É•„É°„É≥„ÉàÂèÇÁÖß**: STRATEGY.md, DatabricksAppsSetup.md
2. **„É≠„Ç∞Á¢∫Ë™ç**: `databricks apps logs --app-name <app-name> --tail 100`
3. **Issue‰ΩúÊàê**: DatabricksAppsIssueStrategy.md „Å´Âæì„Å£„Å¶Issue‰ΩúÊàê

---

**ÊúÄÁµÇÊõ¥Êñ∞**: 2025-10-22
**ÂØæË±°ËÄÖ**: Êó¢Â≠òUIÈñãÁô∫ËÄÖ
**ÂâçÊèê**: Databricks Workspace„Ç¢„ÇØ„Çª„ÇπÊ®©Èôê„ÅÇ„Çä
